---
title: "[ELK Stack-02] Logstash 활용"
category: ELK Stack
tag: Logstash
---

오늘은 **ELK Stack**에서 데이터 수집 엔진인 **Logstash**에 대해 알아보도록 하자. 

Logstash에 대한 소개 및 특정 예시를 통한 활용을 설명하도록 할께.

## Logstash란 무엇인가?

Logstash는 실시간 파이프라인 기능을 가진 오픈 소스 데이터 수집 엔진이야.

<a href="https://i.imgur.com/rHoB6Th"><img src="https://i.imgur.com/rHoB6Th.png" width="700px" title="source: imgur.com" /></a>

위의 그림은 Logstash에서의 데이터 흐름을 표현한 그림으로, Logstash 파이프라인은 **input, filter, output** 단계로 구성되어있어. 각각의 역할은 .

 - **input** : 여러 시스템의 다양한 형태 데이터를 수집하는 역할
 - **filter** : 구문 분석 및 필드 식별을 통해 데이터를 변환하고 정형화하는 역할
 - **output** : 다양한 저장소에 데이터를 출력하는 역할

Logstash를 활용하면 다양한 source에서 데이터를 수집하고 동적으로 변환, 다양한 저장소로 출력할 수 있다는 장점이 있어.

이런 장점은 **플러그인** 기반으로 input/filter/output을 구성할 수 있다는 특성에서 나오게 돼.

200개가 넘는 플러그인 사용이 가능하며, 다른 input/filter/output 플러그인을 조합할 수 있지.

---

## Logstash 파이프라인 구현: (1) .conf 파일 작성

Logstash 파이프라인 구성을 위해서는 **구성(.conf) 파일**을 만들어야해. 해당 포스트에서는 "logstash/config/test.conf"에 생성한다고 가정할께.

### Logstash 구성 파일 형태

```json
## logstash/config/test.conf

input{
  ...
  }

filter{
  ...
  }

output{
  ...
  }
```

input 단계에는 beats/elasticsearch/file/jdbc 등의 입력 플러그인, filter 단계에는 dissect/grok/mutate/split 등의 필터 플러그인, output 단계에는 stdout/csv/elasticsearrch/email 등의 출력 플러그인을 넣을 수 있어.

다양한 플러그인과 그 기능은 [Logstash reference](https://www.elastic.co/guide/en/logstash/current/index.html)에서 확인할 수 있어.

## Logstash 파이프라인 구현: (2) logstash.bat 실행

---

## Logstash 파이프라인 구현 예제

예제 데이터를 수집/변환하여 Elasticsearch 서버로 적재하는 작업을 Logstash를 활용해서 진행해보자. 

### 예제: 일기예보 데이터

다음의 예제 데이터가 "C:/weather_201901" 경로에 "test"라는 파일명으로 존재한다고 하자.

|20190101;서울;주간;아침에는 북서풍이 불어 기온이 낮을 예정입니다.|
|20190101;서울;야간;저녁에는 맑은 날씨를 보입니다.\r\n 밝은 보름달을 볼 수 있습니다.|
|...|
|20190131;전북;주간;거센 눈발이 날릴 예정입니다.\r\n 운전 및 야외활동을 주의하세요.|

위의 일기예보 데이터의 특징은 다음과 같다고 하자.

 - test 파일은 확장자가 없는 파일이다.
 - 파일은 header를 포함하지 않는다. 단, Data;Loc;DayNight;Contents 순서로 적재된다.
 - Contents 필드에는 "\r\n" 줄바꿈 escape 문자가 포함될 수 있다.
 - 매월 1일 0시에 해당 달의 폴더가 생성된다. 예를들어 19년 10월 1일에는 "C:/weather_201910/test"가 생성된다.
 
위에 데이터를 수집해서 Elasticsearch 서버로 보내기 위해 "logstash/config/test.conf" 파일을 만들어보자.

### test.conf 파일 생성

#### input 블락

```json
input {
     file {
         path => ["C:/weather_*/*"]
         start_position => "beginning"
         sincedb_path => "C:/sincedb"
         stat_interval => "10"
         discover_interval => "3600"
         sincedb_write_interval => "15"
     }
}
```

[**file** 플러그인](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html)은 파일에서 직접 데이터를 수집할 때 사용하는 플러그인이야. file 플러그인 안에서 여러 설정들을 할 수 있어.

   + **path**는 파일의 경로이고, "C:/weather_\*/\*"는 C 드라이브 안에 "weather_"로 시작하는 폴더들의 모든 파일을 수집하는 설정이야.
   + **start_position**은 logstash 실행시에 파일의 어디부터 읽을지에 대한 설정이야. "beginning"/"end" 설정이 있어.
      - "beginning" : 파일의 처음부터 읽음.
      - "end" : 파일의 끝에서부터 추가되는 이벤트만 읽음. 이 경우 logstash 실행 이전에 적재된 데이터는 **유실**됨.
   + **sincedb_path**는 파일의 어느 위치까지 읽었는지 정보가 있는 파일 경로이다.
      - logstash 최초 실행시 해당 위치에 파일을 만들고, 파일별로 어디까지 읽었는지 기록한다.
      - logstash가 데이터를 수집함에 따라 주기적으로 업데이트 된다.
      - logstash가 최초 실행시에는 start_position 옵션 적용되고, 재실행부터는 sincedb에 기록된 시점 다음부터 수집.
      - 경로를 "nul"로 하면 sincedb를 만들지 않고, 재실행시에도 start_position 옵션 적용.
   + **stat_interval**은 파일 갱신 확인 주기이다. 예제에서는 10 sec 주기로 확인.
   + **discover_interval**은 파일 패턴에 맞는 파일 생성 확인 주기이다. 예제에서는 3600 sec 주기로 확인.
      - 예제에서는 월 1회 파일이 새로 추가되고, 해당 옵션으로 생성을 체크.
   + **sincedb_write_interval**은 sincedb를 업데이트하는 주기로, 예제에서는 15 sec 이다.
      - 만약 logstash가 데이터 수집 중에 종료되는 경우, 마지막에 어디까지 읽었는지가 sincedb에 업데이트 되지 않을 수 있다. 이런 경우 logstash를 재실행하면 데이터가 **중복**된다.

start_position, sincedb_path, sincedb_write_interval는 logstash의 offset 관리를 위해 잘 이해하고 있는 것이 좋아.
