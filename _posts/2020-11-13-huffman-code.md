---
title: 허프만(Huffman) 코드
category: Information Theory
tag: Huffman code
---

**허프만(Huffman) 코드**는 데이터를 효율적으로 압축하거나 전송할 때 사용하는 코딩 방법이야.

오늘은 허프만 코드가 어떻게 데이터를 압축할 수 있는지 그 원리에 대해 이야기해보자.

## 이진코드(Binary code)

이진코드는 모든 기호를 0 또는 1로 이루어진 이진 숫자 조합으로 나타내는 코딩 방법이야.

우리가 쓰는 디지털 기기에서 모든 기호는 0 또는 1로 이루어진 bit의 조합으로 구성되어있어.

예를들어 A, B, C, D 네 개의 문자는 다음과 같이 2 bit의 이진코드로 표현이 가능해.

 - A $\rightarrow$ 00
 - B $\rightarrow$ 01
 - C $\rightarrow$ 10
 - D $\rightarrow$ 11

우리가 알고 있는 아스키(ASCII) 코드는 128개 기호를 8 bit의 이진수로 표현한 대표적인 이진코드 중 하나야.

## 정보(information)와 정보

정보가 무엇인가? 주위에서 정보라는 용어를 많이 듣는데 막상 정보의 정의를 말하는 것은 쉽지가 않다.

정보의 사전적 의미는 '문제에 도움이 되는 **유용한** 가치가 있는 자료'를 의미해. 하지만 정보 이론에서 정보는 살짝 다른 의미를 가져. 

정보 이론에서 정보란 어떤 특정 사건이 일어났을 때, 그 사건의 발생이 주는 **surprising한 정도**를 의미해.

사건의 유용성 보다는 놀라움이 정보의 양을 측정하는 기준이 되는거지.

예를들어, 오후에 부산에 눈이 온다는 사실을 들었다고 하자. 이 사건은 서울에 사는 나한테는 별로 유용한 정보는 아니지만 놀라운 정도는 크기때문에, 정보 이론에서는 이런 사건은 정보의 양이 큰 사건인거지.

잘 일어나지 않은 사건이 일어난 경우에 놀라움은 더 커지게 돼. 즉, 정보량은 놀라움과 비례하고 확률과는 반비례하지.

정보량이라는 것이 무엇을 의미하는지는 위에서 본 것처럼 정의할 수 있어.

그럼 정보량이 가지고 있어야하는 성질을 몇 가지 정리해보자.

- 자주 발생하는 사건은 낮은 정보량을 가짐.
- 자주 발생하지 않는 사건은 높은 정보량을 가짐.
- 독립적인 두 사건이 동시에 일어나는 사건은 두 사건의 정보량 합으로 표현된다. (additivity)

(3)의 경우는, 부산에 비가 오는 사건과 미국 대선에서 A가 당선되는 사건을 들었다면 그 놀라움의 정도는 순수한 합으로 표현될 수 있을것이야.
