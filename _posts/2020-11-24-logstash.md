---
title: "[ELK Stack-02] Logstash 활용"
category: ELK Stack
tag: Logstash
---

오늘은 **ELK Stack**에서 데이터 수집 엔진인 **Logstash**에 대해 알아보도록 하자. 

Logstash에 대한 소개 및 특정 예시를 통한 활용을 설명하도록 할께.

## Logstash란 무엇인가?

Logstash는 실시간 파이프라인 기능을 가진 오픈 소스 데이터 수집 엔진이야.

<a href="https://i.imgur.com/rHoB6Th"><img src="https://i.imgur.com/rHoB6Th.png" width="700px" title="source: imgur.com" /></a>

위의 그림은 Logstash에서의 데이터 흐름을 표현한 그림으로, Logstash 파이프라인은 **input, filter, output** 단계로 구성되어있어. 각각의 역할은 .

 - **input** : 여러 시스템의 다양한 형태 데이터를 수집하는 역할
 - **filter** : 구문 분석 및 필드 식별을 통해 데이터를 변환하고 정형화하는 역할
 - **output** : 다양한 저장소에 데이터를 출력하는 역할

Logstash를 활용하면 다양한 source에서 데이터를 수집하고 동적으로 변환, 다양한 저장소로 출력할 수 있다는 장점이 있어.

이런 장점은 **플러그인** 기반으로 input/filter/output을 구성할 수 있다는 특성에서 나오게 돼.

200개가 넘는 플러그인 사용이 가능하며, 다른 input/filter/output 플러그인을 조합할 수 있지.

---

## Logstash 구현: (1) .conf 파일 작성

Logstash 파이프라인 구성을 위해서는 **구성(.conf) 파일**을 만들어야해. 해당 포스트에서는 "logstash/config/test.conf"에 생성한다고 가정할께.

### Logstash 구성 파일 형태

```json
## logstash/config/test.conf
input{
  ...
  }

filter{
  ...
  }

output{
  ...
  }
```

input 단계에는 beats/elasticsearch/file/jdbc 등의 입력 플러그인, filter 단계에는 dissect/grok/mutate/split 등의 필터 플러그인, output 단계에는 stdout/csv/elasticsearrch/email 등의 출력 플러그인을 넣을 수 있어.

다양한 플러그인과 그 기능은 [Logstash reference](https://www.elastic.co/guide/en/logstash/current/index.html)에서 확인할 수 있어.

## Logstash 구현: (2) logstash.bat 실행

test.conf 파일을 다 만들었으면, /bin/logstash.bat 파일로 logstash를 구동할 수 있어.

bin 폴더 경로가 시스템 환경 변수에 들어있다고 가정하고, 명령프롬프트(cmd)에서 다음을 입력해보자.

```powershell
logstash -r -f /config/test.conf
```

앞에 logstash는 logstash.bat을 구동하는 명령어, -r은 .conf 파일이 변경될 때 자동으로 reload하는 옵션, -f는 .conf 파일을 load하는 옵션이야.

-f 뒤에 .conf 파일 경로를 넣으면 해당 파일에 명시된 대로 logstash가 구동하게 돼.

---

## Logstash 구현 예제

예제 데이터를 수집/변환하여 Elasticsearch 서버로 적재하는 작업을 Logstash를 활용해서 진행해보자. 

### 예제: 일기예보 데이터

다음의 예제 데이터가 "C:/weather_201901" 경로에 "test"라는 파일명으로 존재한다고 하자.

|20190101;서울;주간;아침에는 북서풍이 불어 기온이 낮을 예정입니다.|
|20190101;서울;야간;저녁에는 맑은 날씨를 보입니다.\n 밝은 보름달을 볼 수 있습니다.|
|...|
|20190131;전북;주간;거센 눈발이 날릴 예정입니다.\r\n 운전 및 야외활동을 주의하세요.|

위의 일기예보 데이터의 특징은 다음과 같다고 하자.

 - test 파일은 확장자가 없는 파일이다.
 - 파일은 header를 포함하지 않는다. 단, Data;Loc;DayNight;Contents 순서로 적재된다.
 - Contents 필드에는 "\r\n" 또는 "\n" 줄바꿈 escape 문자가 포함될 수 있다.
 - 매월 1일 0시에 해당 달의 폴더가 생성된다. 예를들어 19년 10월 1일에는 "C:/weather_201910/test"가 생성된다.
 
위에 데이터를 수집해서 Elasticsearch 서버로 보내기 위해 "logstash/config/test.conf" 파일을 만들어보자.

### test.conf 파일 생성

#### input 블락

```json
input {
     file {
         path => ["C:/weather_*/*"]
         start_position => "beginning"
         sincedb_path => "C:/sincedb"
         stat_interval => "10"
         discover_interval => "3600"
         sincedb_write_interval => "15"
     }
}
```

[**file** 플러그인](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html)은 파일에서 직접 데이터를 수집할 때 사용하는 플러그인이야. file 플러그인 안에서 여러 설정들을 할 수 있어.

   + **path**는 파일의 경로이고, "C:/weather_\*/\*"는 C 드라이브 안에 "weather_"로 시작하는 폴더들의 모든 파일을 수집하는 설정이야.
   + **start_position**은 logstash 실행시에 파일의 어디부터 읽을지에 대한 설정이야. "beginning"/"end" 설정이 있어.
      - "beginning" : 파일의 처음부터 읽음.
      - "end" : 파일의 끝에서부터 추가되는 이벤트만 읽음. 이 경우 logstash 실행 이전에 적재된 데이터는 **유실**됨.
   + **sincedb_path**는 파일의 어느 위치까지 읽었는지 정보가 있는 파일 경로를 설정하는 거야.
      - logstash 최초 실행시 해당 위치에 파일을 만들고, 파일별로 어디까지 읽었는지 기록.
      - logstash가 데이터를 수집함에 따라 주기적으로 업데이트 됨.
      - logstash가 최초 실행시에는 start_position 옵션 적용되고, 재실행부터는 sincedb에 기록된 시점 다음부터 수집.
      - 경로를 "nul"로 하면 sincedb를 만들지 않고, 재실행시에도 start_position 옵션 적용.
   + **stat_interval**은 파일 갱신 확인 주기로, 예제에서는 10 sec 주기로 설정했어.
   + **discover_interval**은 파일 패턴에 맞는 파일 생성 확인 주기로, 예제에서는 3600 sec 주기로 설정했어.
      - 예제에서는 월 1회 파일이 새로 추가되고, 해당 옵션으로 생성을 체크.
   + **sincedb_write_interval**은 sincedb를 업데이트하는 주기로, 예제에서는 15 sec 주기로 설정했어.
      - 만약 logstash가 데이터 수집 중에 종료되는 경우, 마지막에 어디까지 읽었는지가 sincedb에 업데이트 되지 않을 수 있다. 이런 경우 logstash를 재실행하면 데이터가 **중복**됨.

start_position, sincedb_path, sincedb_write_interval는 logstash의 offset 관리를 위해 잘 이해하고 있는 것이 좋아.



#### filter 블락

```json
filter {
	dissect {
		mapping => { "message" => "%{Date};%{Loc};%{DayNight};%{Contents}" }
		remove_field => ["message", "@timestamp", "@version", "path", "host"]
	}

	mutate => {
		gsub => ["Contents", "\\n","
"]
		gsub => ["Contents", "\\r", ""]
	}

	date => {
		match => ["Date", "YYYYMMdd"]
		target => "Date"
	}
}
```

[**dissect** 플러그인](https://www.elastic.co/guide/en/logstash/current/plugins-filters-dissect.html)은 이벤트를 필드와 구분자 포맷을 명시해서 필드를 분해하는 플러그인이야. 이벤트의 구분자가 한 종류가 아니라 여러 종류일때도 적용 가능해. 비슷한 역할을 하는 **grok** 플러그인과 달리 regular expression을 사용할 수는 없지만 훨씬 빠르게 작동해. 

  + **mapping**은 message를 필드들로 나누는 기준을 명시하는 옵션이야. **%{필드명}**과 **구분자**를 포맷에 맞게 각각의 위치에 명시해서 사용할 수 있어.
  + **remove_field**는 불필요한 필드를 제거하는 옵션이야. 필드명을 넣어서 제거할 수 있어.
    - logstash는 데이터 수집시에 @timestamp, @version, path, host 등의 필드가 자동 생성돼.
    - 데이터에 포함된 필드만 사용하기 위해, 위의 4개 필드를 제거.
 
 
[**mutate** 플러그인](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html)은 필드를 수정/변형할 때 사용하는 플러그인이야. 필드명 수정, 필드 제거, 필드 수정 등을 할 수 있어.
  
  + **gsub**은 regular expression을 사용해서 해당하는 내용을 대체할 문자열로 변경하는 옵션이야.
    - Content 필드에 포함된 "\n"과 "\r\n"은 logstash가 데이터를 읽을 때, 줄바꿈이 아닌 문자열로 인식.
    - 문자열로 인식한 "\n"과 "\r\n"을 다시 줄바꿈으로 바꾸기 위해 사용.

[**date** 플러그인](https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html)은 필드를 구문 분석하여, date/timestamp로 만드는데 사용하는 플러그인이야. 만약 해당 플러그인이 없다면 logstash는 message를 읽는 시간을 timestamp로 저장해.

  + **match**는 날짜/시간에 해당되는 필드를 구문 분석하기 위한 옵션이야.
    - 예제에서는 Date 필드가 "20190101"의 형태이므로, "YYYYMMdd" 형식을 사용.
  + **target**은 날짜/시간 형식으로 변환한 필드를 저장하는 옵션이야.
    - target을 명시하지 않으면 변환된 필드는 @timestamp 필드로 저장.
    - 예제에서는 변환된 필드를 다시 Date 필드에 저장.



#### output 블락

```json
output {
	stdout {
		codec => rubydebug
	}
  
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "weather"
  }
}
```


[**stdout** 플러그인](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-stdout.html)은 input/filter를 지난 후의 데이터를 콘솔창에서 프린트하는 플러그인이야. 결과를 바로 확인하기 때문에 구성 파일에 대한 **debugging** 작업을 편리하게 할 수 있어. 
  
  + **codec**은 처리된 데이터를 적절한 형태로 출력하기 위한 옵션이야.
    - default 값은 "rubydebug"로 json 포맷으로 출력하는 키워드.
  

[**elasticsearch** 플러그인](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html)은 kibana 사용을 목적으로 데이터를 elasticsearch에 올릴 때 사용하는 플러그인이야.
  
  + **hosts**은 데이터를 받을 elasticsearch 주소를 설정하는 옵션이야.
    - elasticsearch가 logstash가 구동되는 동일한 곳에 있는 경우, **"localhost:9200"** 으로 작성.
  + **index**는 데이터가 들어갈 index 명을 의미해.
    - 예제에서 처리된 데이터는 "weather"라는 이름을 갖는 index에 들어가게 됨.

### test.conf 구성 파일 구동

위와 같이 test.conf 의 input/filter/output 구성이 완료되면, 명령프롬프트에서 다음의 명령어로 구성 파일을 구동할 수 있어.

```powershell
logstash -r -f /config/test.conf
```
실제로 구성 파일을 구동하게 되면 다음 2 가지 결과를 볼 수 있어.

  + 이벤트가 filter를 거쳐서 처리되어 **명령프롬프트창에 json 형태로 출력**
  + 이벤트가 filter를 거쳐서 동일 위치 **elasticsearch의 weather index에 적재**
  
---

## (Extra) 두 개 이상의 구성 파일 구동 (pipelines.yml 이용)

위의 예제에서 하나의 구성 파일을 생성해서 구동하는 방법을 진행했어.

만약 우리가 **두 개 이상의 구성 파일을 동시에 구동**하고 싶을 때는 어떻게 하면 될까?

예를들어, 다음의 두 가지 작업을 진행한다고 생각해보자.

  + 일기예보 데이터를 수집/가공해서 elasticsearch의 "weather" index에 적재
  + 쇼핑몰 후기를 수집/가공해서 elasticsearch의 "shopping" index에 적재

위의 경우, 서로 다른 input/filter/output이 필요하므로 두 개의 구성 파일을 생성해야해.

이런 때 **pipelines.yml** 파일을 이용하면 **두 개 이상의 파이프라인 구성 파일을 묶어서 실행**할 수 있어.

일단 위의 두 개의 파이프라인을 명시한 구성 파일이 각각 "test1.conf", "test2.conf"로 생성되어있다고 하자.

그럼 다음의 두 가지 절차로  "test1.conf", "test2.conf"를 묶어서 구동할 수 있어.

1. config/pipelines.yml 작성 

    ```json
    - pipeline.id: test1
      path.config: "test1.conf"
    - pipeline.id: test2
      path.config: "test2.conf"
    ``` 
    + pipeline.id는 각각의 구성 파일의 id, path.config는 구성 파일의 경로를 입력.

2. 명령프롬프트에서 다음의 명령어로 pipelines.yml 을 구동

    ```powershell
    logstash -r
    ```
    + logstash.bat 구동시, "-f [conf 파일명]"을 명시하지 않으면 pipelines.yml 이 구동됨.
    
