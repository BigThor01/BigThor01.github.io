---
title: "[Information Theory-01] Information & Entropy"
category: Information Theory
tag: Entropy
---

## 정보(information)와 정보량

정보가 무엇인가? 주위에서 정보라는 용어를 많이 듣는데 막상 정보의 정의를 말하는 것은 쉽지가 않다.

정보의 사전적 의미는 '문제에 도움이 되는 **유용한** 가치가 있는 자료'를 의미해. 하지만 정보 이론에서 정보는 살짝 다른 의미를 가져. 

정보 이론에서 정보란 어떤 특정 사건이 일어났을 때, 그 사건의 발생이 주는 **surprising한 정도**를 의미해.

사건의 유용성 보다는 놀라움이 정보의 양을 측정하는 기준이 되는거지.

예를들어, 오후에 부산에 눈이 온다는 사실을 들었다고 하자. 이 사건은 서울에 사는 나한테는 별로 유용한 정보는 아니지만 놀라운 정도는 크기때문에, 정보 이론에서는 이런 사건은 정보의 양이 큰 사건인거지.

잘 일어나지 않은 사건이 일어난 경우에 놀라움은 더 커지게 돼. 즉, 정보량은 놀라움과 비례하고 확률과는 반비례하지.

정보량이라는 것이 무엇을 의미하는지는 위에서 본 것처럼 정의할 수 있어.

그럼 정보량이 가지고 있어야하는 성질을 몇 가지 정리해보자.

- 자주 발생하는 사건은 낮은 정보량을 가짐.
- 자주 발생하지 않는 사건은 높은 정보량을 가짐.
- 독립적인 두 사건이 동시에 일어나는 사건은 두 사건의 정보량 합으로 표현된다. (additivity)

(3)의 경우는, 부산에 비가 오는 사건과 미국 대선에서 A가 당선되는 사건을 들었다면 그 놀라움의 정도는 순수한 합으로 표현될 수 있을것이야.
