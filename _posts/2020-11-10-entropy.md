---
title: "[Information Theory-01] Information & Entropy"
category: Information Theory
tag: Entropy
---

## 정보(information)와 정보량

정보가 무엇인가? 주위에서 정보라는 용어를 많이 듣는데 막상 정보의 정의를 말하는 것은 쉽지가 않다.

정보의 사전적 의미는 '문제에 도움이 되는 **유용한** 가치가 있는 자료'를 의미해. 하지만 정보 이론에서 정보는 살짝 다른 의미를 가져. 

정보 이론에서 정보란 어떤 특정 사건이 일어났을 때, 그 사건의 발생이 주는 **surprising한 정도**를 의미해.

사건의 유용성 보다는 놀라움이 정보의 양을 측정하는 기준이 되는거지.

예를들어, 오후에 부산에 눈이 온다는 사실을 들었다고 하자. 이 사건은 서울에 사는 나한테는 별로 유용한 정보는 아니지만 놀라운 정도는 크기때문에, 정보 이론에서는 이런 사건은 정보의 양이 큰 사건인거지.

잘 일어나지 않은 사건이 일어난 경우에 놀라움은 더 커지게 돼. 즉, 정보량은 놀라움과 비례하고 확률과는 반비례하지.

정보량이라는 것이 무엇을 의미하는지는 위에서 본 것처럼 정의할 수 있어.

그럼 정보량이 가지고 있어야하는 성질을 몇 가지 정리해보자.

- 자주 발생하는 사건은 낮은 정보량을 가짐.
- 자주 발생하지 않는 사건은 높은 정보량을 가짐.
- 독립적인 두 사건이 동시에 일어나는 사건은 두 사건의 정보량 합으로 표현된다. (additivity)

(3)의 경우는, 부산에 비가 오는 사건과 미국 대선에서 A가 당선되는 사건을 들었다면 그 놀라움의 정도는 순수한 합으로 표현될 수 있을것이야.






2020-11-19



## 정보

## 정보량

## 의외성

## 엔트로피

## 메시지의 전달


메시지(특정 사건)에 포함된 정보량이 있다. > 무한히 시행할 때 정보량의 기댓값이 엔트로피다.

정보량이란 해당 메시지가 발생할 확률의 음의 로그로 정의한다. 

확률의 음의 로그(log 2)로 정의하는 이유는, 의외성과 가산성을 반영

동전 1번 던지기가 1 섀넌이면, m번 던지는 것은 m 섀넌


n개의 결과가 있는 균등확류룬포에서 결과를 이진코드로 표현하려면 log_2 (n) 비트가 필요하다.

위에서 정의한 섀년 엔트로피와 정확히 동일

만약 하나의 사건이 다른 사건보다 발생확률이 높다면, 정보량은 적다.

평균을 내면 정보량이 큰 것은 더 적은 확률이므로 엔트로피가 작아짐.



엔트로피는 소스의 분포가 알려져있을때, 분포를 수치화한다. 단 개별 메시지의 의미 자체는 고려하지 않음

엔트로피는 불확실성 또는 무질서도.

섀넌 엔트로피는 정보 소스를 무손실 인코딩 압축할때 가능한 최상의 평균 길이의 한계치를 제공해준다. (비트와의 연결점)




정보 이론에서 기본 가정은 정보를 더 많이 알수록 새롭게 알 수 있는 정보가 적어진다는 것.

기대하지 못한 사건이 일어났을때 더 많은 정보를 제공한다. 즉, 정보량(information content)는 확률에 반비례

--> 정보를 많이 알수록 새롭게 알 수 있는 정보가 줄어든다? 그래서 뭐
--> 기대치 못한 사건의 정보량이 더 크다 = 정보량과 확률은 반비례



--> 엔트로피는 정보량의 기댓값.  어떤 상태의 불확실성(?)

아무 사전 정보가 없는 첫 선거  -> 결과가 무엇이 나올지 모른다. -> 첫선거라는 확률공간에서 불확실성이 크다

첫선거를 치루고, 내일 다시 같은 후보로 선거를 하면 -> 첫선거의 결과를 알고 있을때 두번째 선거 확률공간에서 불확실성은 작아짐

불확실성이 크다는 것은 결과가 무엇일지 추측조차 하지 못하는 상황을 의미한다. 결과를 예상하기가 어렵다.

불확실성의 정도를 의미하는 엔트로피는 다음 성질을 만족해야지 : 사건들의 확률이 거의 동등 & 경우의 수가 많을수록 엔트로피가 커야한다.


어떤 확률공간에서 사건의 발생으로부터 정보를 얻을 수 있는 가능성을 표현하는 정보 엔트로피



 불확실성을 나타내는 엔트로피라는 measure가 있따하자.

검은색으로만 깔린 타일에서 하나 선택
검은색/흰색 무작위로 깔린 타일에서 하나 선택


2)에서 뭐가 나올지 아예 예측이 불가하다 -> 불확실성이 크다. = 엔트로피가 크다.
