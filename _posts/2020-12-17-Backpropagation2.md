---
title: "딥러닝 기초 - 역전파 알고리즘(2)"
category: Deep learning
tag: ["Bagpropagation"]
---

지난 포스팅에서 딥러닝 네크워크에서 가중치의 gradient 를 구하기 위한 역전파 알고리즘에 대해 직관적인 이해를 지난 포스팅에 대해 이야기했어.

오늘은 다음 영상을 참고해서 수식을 통해 실제로 gradient 를 구해보는 작업에 대해 정리해볼께.

 - [역전파 미적분 | 심층 학습, 4장, 3Blue1Brown youtube](https://youtu.be/tIeHLnjs5U8)


자세한 내용을 보기 전에, 오늘 다룰 내용을 간략히 소개하고 시작할께.

Gradient 를 계산하기 위해서는 일단 학습 예제를 네트워크를 따라 출력까지 보내는 forward propagation 을 수행해서 뉴런들의 활성화 값을 구하게 돼. 

모든 활성화 값을 구하고 나면, cost 를 각각의 활성화 값으로 미분한 미분계수를 출력층 부터 거꾸로 올라가면서 구하게 돼. 
이렇게 거꾸로 올라가면서 계산을 하는 이유는어떤 층의 활성화값로 미분한 값을 구할 때, 그 다음 층의 활성화로 미분한 미분계수가 포함되기 때문이야.

위와 같이 역전파 알고리즘을 통해 모든 뉴런의 활성화의 미분을 구할 수 있고 이것을 활용하면 가중치의 미분계수 즉, gradient 를 구할 수 있게 돼.

Cost 에 대한 가중치의 gradient 는 가중치 변동이 cost 를 얼마나 움직이는지를 알려주는 수치야. 미분값이 크면 가중치를 조금만 변화시켜도 cost 가 많이 움직이고, 반대로 미분값이 작으면 cost 는 조금만 움직여.

이는 negative gradient 방향($-\nabla C$) 이 cost 가 가장 빨리 감소하는 방향이라는 사실과 연결해서 생각할 수 있어.

이제 간단한 예제를 통해 역전파 알고리즘을 적용해서 가중치의 gradient 를 구해보도록 하자.

---
## 간단한 네트워크의 gradient 계산

